{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRI0k_iw4T5v"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "        # open the file\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "\n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text"
      ],
      "metadata": {
        "id": "UZ1fGquX4fr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ],
      "metadata": {
        "id": "vq-Wmemd4jxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_text(\"/content/deu.txt\")\n",
        "eng_pes = to_lines(data)\n",
        "eng_pes = array(eng_pes)"
      ],
      "metadata": {
        "id": "oEIA6v4jgULT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pes"
      ],
      "metadata": {
        "id": "Qhiho0PVhAmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_pes = eng_pes[:50000,:]"
      ],
      "metadata": {
        "id": "gR91hWyM4kQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "eng_pes[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_pes[:,0]]\n",
        "eng_pes[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in eng_pes[:,1]]\n",
        "\n",
        "eng_pes\n"
      ],
      "metadata": {
        "id": "aRW35fNr8aiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert text to lowercase\n",
        "for i in range(len(eng_pes)):\n",
        "    eng_pes[i,0] = eng_pes[i,0].lower()\n",
        "    eng_pes[i,1] = eng_pes[i,1].lower()"
      ],
      "metadata": {
        "id": "viIWH_YZ9VIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# empty lists\n",
        "eng_l = []\n",
        "pes_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in eng_pes[:,0]:\n",
        "      eng_l.append(len(i.split()))\n",
        "\n",
        "for i in eng_pes[:,1]:\n",
        "      pes_l.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'eng':eng_l, 'pes':pes_l})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Gph3bbL29h2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "U-9cGF7O-Lun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(eng_pes[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "metadata": {
        "id": "Unrw0P43-pB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare Persian tokenizer\n",
        "pes_tokenizer = tokenization(eng_pes[:, 1])\n",
        "pes_vocab_size = len(pes_tokenizer.word_index) + 1\n",
        "\n",
        "pes_length = 8\n",
        "print('Persian Vocabulary Size: %d' % pes_vocab_size)"
      ],
      "metadata": {
        "id": "wWAmniTU-uU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq"
      ],
      "metadata": {
        "id": "fYveyNps-9dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "gq-wxNgBDCya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into train and test set\n",
        "train, test = train_test_split(eng_pes, test_size=0.2, random_state = 12)"
      ],
      "metadata": {
        "id": "y1kcKJ6kDCOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(pes_tokenizer, pes_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(pes_tokenizer, pes_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n"
      ],
      "metadata": {
        "id": "8z3-jHUsDISv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import MultiHeadAttention"
      ],
      "metadata": {
        "id": "vwV3u9IM4qfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import MultiHeadAttention\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    inputs = layers.Input(shape=(in_timesteps,))\n",
        "    embedded_inputs = layers.Embedding(input_dim=in_vocab, output_dim=units, input_length=in_timesteps, mask_zero=True)(inputs)\n",
        "\n",
        "    # Calculate query and key\n",
        "    query = keys = layers.Dense(units)(embedded_inputs)\n",
        "\n",
        "    # Adding the self-attention mechanism\n",
        "    attention_output = MultiHeadAttention(num_heads=2, key_dim=units)(query, keys)\n",
        "\n",
        "    lstm1 = layers.LSTM(units, return_sequences=False)(attention_output)\n",
        "    repeated = layers.RepeatVector(out_timesteps)(lstm1)\n",
        "\n",
        "    lstm2 = layers.LSTM(units, return_sequences=True)(repeated)\n",
        "    outputs = layers.Dense(out_vocab, activation='softmax')(lstm2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "SSpyRiM06nop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    inputs = layers.Input(shape=(in_timesteps,))\n",
        "    embedding = layers.Embedding(input_dim=in_vocab, output_dim=units, mask_zero=True)(inputs)\n",
        "\n",
        "    # Adding the self-attention mechanism\n",
        "    attention = MultiHeadAttention(num_heads=2, key_dim=units)\n",
        "    query = attention([embedding, embedding])\n",
        "\n",
        "    lstm1 = layers.Bidirectional(LSTM(units, return_sequences=False))(query)\n",
        "    repeated = layers.RepeatVector(out_timesteps)(lstm1)\n",
        "\n",
        "    lstm2 = layers.LSTM(units, return_sequences=True)(repeated)\n",
        "    outputs = layers.Dense(out_vocab, activation='softmax')(lstm2)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "FSUL9n1T41r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model compilation\n",
        "model = define_model(pes_vocab_size, eng_vocab_size, pes_length, eng_length, 512)"
      ],
      "metadata": {
        "id": "6n_w5NziDIvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "h0Ib5efDD6q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'model.multi_head.13Nov'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint],\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "ft0KyZMSEfIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SSQfi6kXUTiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model = load_model('model.h1.24_Oct_29_2')\n",
        "# Pred\n",
        "preds = model.predict(testX.reshape((testX.shape[0],testX.shape[1])))\n",
        "# Convert predictions to classes\n",
        "preds_classes = np.argmax(preds, axis=-1)"
      ],
      "metadata": {
        "id": "SlX4Y-mCUZwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_classes"
      ],
      "metadata": {
        "id": "vDkukw-Aavn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "zPzDcUi0bQjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds_classes:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "    preds_text.append(' '.join(temp))"
      ],
      "metadata": {
        "id": "_MHsB0z6bLiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test[:,0]))\n",
        "print(len(preds_text))"
      ],
      "metadata": {
        "id": "ANKf7UMZcayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
      ],
      "metadata": {
        "id": "qkIbCxjQcLES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print 15 rows randomly\n",
        "pred_df.head(15)"
      ],
      "metadata": {
        "id": "Bz9WXaIJcnki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Specify the path to your .pth file\n",
        "pth_file_path = '/content/speakers (2).pth'\n",
        "\n",
        "# Load the model or data from the .pth file\n",
        "loaded_data = torch.load(pth_file_path)\n",
        "loaded_data_1 = torch.load(\"/content/speakers (4).pth\")\n",
        "# If it's a model, you can access its components like this:\n",
        "# model = loaded_data['model']\n",
        "\n",
        "# If it's a dictionary containing other data, you can access it like this:\n",
        "# some_data = loaded_data['your_key']\n",
        "\n",
        "# If it's a single tensor, you can directly use it like this:\n",
        "# tensor = loaded_data\n",
        "\n",
        "# You can also specify a device (e.g., 'cpu' or 'cuda') to load the data onto.\n",
        "# For example, if you want to load the model on a specific device:\n",
        "# model = torch.load(pth_file_path, map_location='cuda:0')\n"
      ],
      "metadata": {
        "id": "p_B4Iqjnpm-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data"
      ],
      "metadata": {
        "id": "79X-o_RCpzWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_dict = {'speaker-{}'.format(v): v for k, v in loaded_data.items()}\n",
        "\n",
        "# Print new dictionary\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "XHEGfTXeiiSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.save(new_dict, 'speaker.pth')"
      ],
      "metadata": {
        "id": "IcsutAUTinQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_data_1"
      ],
      "metadata": {
        "id": "vOvpTBxUhPUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}